import math
import os
import sys
from pathlib import Path
from collections import Counter
from tokenizers import Tokenizer
try:
    import tiktoken
    USE_TIKTOKEN = True
except ImportError:
    USE_TIKTOKEN = False
    from transformers import AutoTokenizer

def calculate_unigram_entropy(tokenizer_type, tokenizer_obj, texts):
    token_counts = Counter()
    total_token_len = 0
    total_words = 0
    
    for text in texts:
        if tokenizer_type == "amb":
            ids = tokenizer_obj.encode(text).ids
            tokens = [str(i) for i in ids]
        elif tokenizer_type == "tiktoken":
            tokens = tokenizer_obj.encode(text)
        elif tokenizer_type == "hf":
            tokens = tokenizer_obj.encode(text)
        
        token_counts.update(tokens)
        total_token_len += len(tokens)
        total_words += len(text.split())

    if total_token_len == 0:
        return 0, 0, 0

    # Entropy in bits per token
    entropy = -sum((count/total_token_len) * math.log2(count/total_token_len) 
                   for count in token_counts.values())
    
    # Fertility: tokens per word
    fertility = total_token_len / max(total_words, 1)
    
    return entropy, fertility, total_token_len

def run_experiments():
    print("ЁЯФм --- AMB Quick Experiments --- ЁЯФм\n")
    
    # Load AMB Tokenizer
    amb_path = "models/amb_tokenizer/tokenizer.json"
    if not os.path.exists(amb_path):
        print(f"тЭМ Error: {amb_path} not found. Train with a small sample first.")
        return
    
    amb_tokenizer = Tokenizer.from_file(amb_path)
    
    if USE_TIKTOKEN:
        gpt4_tokenizer = tiktoken.get_encoding("cl100k_base")
        comp_engine = gpt4_tokenizer
        comp_type = "tiktoken"
    else:
        print("тЪая╕П tiktoken not found, using GPT-2 HF as baseline.")
        comp_engine = AutoTokenizer.from_pretrained("gpt2")
        comp_type = "hf"
    
    # Try to load from corpus if available for better stats
    corpus_path = "data/cleaned/tamil_corpus.txt"
    if os.path.exists(corpus_path):
        with open(corpus_path, "r", encoding="utf-8") as f:
            corpus_text = [l.strip() for l in f.readlines() if l.strip()]
            if len(corpus_text) > 1000:
                import random
                test_sentences = random.sample(corpus_text, 1000)
                print(f"тЬЕ Sampled 1000 sentences from {corpus_path}")
            else:
                test_sentences = corpus_text
    else:
        # Fallback to defaults
        test_sentences = [
            "роЗроирпНродро┐роп роЕро░роЪро┐ропро▓роорпИрокрпНрокрпБроЪрпН роЪроЯрпНроЯроорпН роЕройрпИро╡ро░рпБроХрпНроХрпБроорпН роЪроородрпНродрпБро╡родрпНродрпИ роЙро▒рпБродро┐роЪрпЖропрпНроХро┐ро▒родрпБ.",
            "родрооро┐ро┤рпНроиро╛роЯрпБ роЕро░роЪрпБ роХро▓рпНро╡ро┐родрпН родрпБро▒рпИропро┐ро▓рпН рокро▓рпНро╡рпЗро▒рпБ роЪрпАро░рпНродро┐ро░рпБродрпНродроЩрпНроХро│рпИ роорпЗро▒рпНроХрпКрогрпНроЯрпБ ро╡ро░рпБроХро┐ро▒родрпБ.",
            "роЪрпЖропро▒рпНроХрпИ роирпБрогрпНрогро▒ро┐ро╡рпБ родрпКро┤ро┐ро▓рпНроирпБроЯрпНрокроорпН ро╡рпЗроХрооро╛роХ ро╡ро│ро░рпНроирпНродрпБ ро╡ро░рпБроХро┐ро▒родрпБ.",
            "ропро╛родрпБроорпН роКро░рпЗ ропро╛ро╡ро░рпБроорпН роХрпЗро│ро┐ро░рпН родрпАродрпБроорпН роиройрпНро▒рпБроорпН рокро┐ро▒ро░рпНродро░ ро╡ро╛ро░ро╛.",
            "рооро┐ройрпНройрогрпБро╡ро┐ропро▓рпН родрпБро▒рпИ рооро╛ро▒рпНро▒роЩрпНроХро│рпИ роЪроирпНродро┐родрпНродрпБ ро╡ро░рпБроХро┐ро▒родрпБ."
        ]
    
    print(f"ЁЯУК [Experiment 1] Entropy & Fertility Showdown (vs {comp_type})")
    amb_ent, amb_fert, amb_toks = calculate_unigram_entropy("amb", amb_tokenizer, test_sentences)
    gpt_ent, gpt_fert, gpt_toks = calculate_unigram_entropy(comp_type, comp_engine, test_sentences)
    
    print(f"{'Metric':<15} | {'AMB':<10} | {'GPT-4':<10} | {'Improvement'}")
    print("-" * 55)
    print(f"{'Fertility':<15} | {amb_fert:<10.2f} | {gpt_fert:<10.2f} | {((gpt_fert/amb_fert)-1)*100:>7.1f}% better")
    print(f"{'Entropy (bits)':<15} | {amb_ent:<10.2f} | {gpt_ent:<10.2f} | {((gpt_ent/amb_ent)-1)*100:>7.1f}% dense")
    print(f"{'Total Tokens':<15} | {amb_toks:<10} | {gpt_toks:<10} | {gpt_toks-amb_toks} saved")
    print("\n")

    # 2. Code-Mixing Robustness
    print("ЁЯТ╗ [Experiment 2] Code-Mixing Robustness")
    code_mixed_samples = [
        "Python-ро▓ coding роЪрпЖропрпНроХро┐ро▒рпЗройрпН",
        "AI-ропро┐ройрпН future рооро┐роХро╡рпБроорпН bright-роЖроХ роЗро░рпБроХрпНроХро┐ро▒родрпБ",
        "Netflix-роЗро▓рпН Tamil movies рокро╛ро░рпНроХрпНроХро┐ро▒рпЗройрпН"
    ]
    
    for sample in code_mixed_samples:
        encoded = amb_tokenizer.encode(sample)
        # Check for cross-script tokens manually
        tokens = [amb_tokenizer.decode([i]) for i in encoded.ids]
        leaky = any(any(0x0B80 <= ord(c) <= 0x0BFF for c in t) and any(c.isascii() and c.isalpha() for c in t) for t in tokens)
        print(f"Sample: {sample}")
        print(f"Tokens: {' | '.join(tokens)}")
        print(f"Leaky:  {'тЭМ YES' if leaky else 'тЬЕ NO'}")
    print("\n")

    # 3. Morpheme Boundary Stress Test (Oblique stems)
    print("ЁЯПЫя╕П [Experiment 3] Morpheme Stress Test (Linguistic Nuance)")
    critical_word = "роЪрпЖройрпНройрпИропро┐ро▓ро┐ро░рпБроирпНродрпБродро╛ройрпН"
    # The user noted "роЪрпЖройрпНройрпИропро┐" + "ро▓ро┐ро░рпБроирпНродрпБ" + "родро╛ройрпН" is slightly off.
    # It should ideally be "роЪрпЖройрпНройрпИ" + "ропро┐ро▓рпН" + "роЗро░рпБроирпНродрпБ" + "родро╛ройрпН"
    
    encoded = amb_tokenizer.encode(critical_word)
    token_texts = [amb_tokenizer.decode([i]) for i in encoded.ids]
    print(f"Word:   {critical_word}")
    print(f"Splits: {' | '.join(token_texts)}")
    
if __name__ == "__main__":
    run_experiments()
